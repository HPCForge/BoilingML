torch_dataset_name: temp_input_dataset 

# torch distributed does not support complex parameters
distributed: False

train:
  max_epochs: 250 
  batch_size: 4
  shuffle_data: True
  time_window: 5
  future_window: 5
  push_forward_steps: 1
  use_coords: True
  noise: True
  downsample_factor: 0.5

model:
  model_name: gfno
  modes: 32
  width: 128
  reflection: False

optimizer:
  initial_lr: 1e-3
  weight_decay: 0.01

lr_scheduler:
  factor: 0.5
  patience: 75
  min_lr: 1e-8
